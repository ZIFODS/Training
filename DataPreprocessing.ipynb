{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72ecd52",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "**Outline**\n",
    "* [Required python libraries](#Required-python-libraries)\n",
    "* [Dataset overall characteristics](#Dataset-and-its-overall-characteristics)\n",
    "* [Missing values](#Missing-Values)\n",
    " * [Deleting the column with missing data](#Deleting-the-column-with-missing-data)\n",
    " * [Deleting the row with missing data](#Deleting-the-row-with-missing-data)\n",
    " * [Filling the Missing Values – Imputation](#Filling-the-Missing-Values-%E2%80%93-Imputation)\n",
    "      * [Filling with regression model](#Filling-with-a-Regression-Model)\n",
    "      * [Filling with KNN](#Filling-with-KNN)\n",
    "* [Data Transformation](#Data-Transformation)\n",
    " * [Clipping](#Clipping)\n",
    " * [Basic Transformation](#Basic-transformation)\n",
    " * [Scaling Techniques](#Scaling-Techniques)\n",
    "      * [MinMax Scaler](#MinMax-Scaler)\n",
    "      * [Standard Scaler](#Standard-Scaler)\n",
    "      * [MaxAbsScaler](#MaxAbsScaler)\n",
    "      * [Robust Scaler](#Robust-Scaler)\n",
    "      * [Quantile Transformer Scaler](#Quantile-Transformer-Scaler)\n",
    " * [Transforming non-normal data](#Transformating-non-normal-data)\n",
    "      * [Skewness and Kurtosis](#Skewness-and-Kurtosis)\n",
    "      * [Squared root transformation](#Square-root-transformation)\n",
    "      * [Log transformation](#Log-transformation)\n",
    "      * [Box-Cox Transformation](#Box-Cox-Transformation)\n",
    "* [Exercise](#Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a13b1",
   "metadata": {},
   "source": [
    "## Required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37360cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\44755\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\44755\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\44755\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\44755\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\44755\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\44755\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\44755\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from scipy) (1.20.3)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\44755\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from statsmodels) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from statsmodels) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.21 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from statsmodels) (1.3.4)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas>=0.21->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\44755\\anaconda3\\lib\\site-packages (from pandas>=0.21->statsmodels) (2021.3)\n",
      "Requirement already satisfied: six in c:\\users\\44755\\anaconda3\\lib\\site-packages (from patsy>=0.5->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary python libraries\n",
    "import sys\n",
    "## for data\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install statsmodels\n",
    "!{sys.executable} -m pip install sklearn.preprocessing\n",
    "!{sys.executable} -m pip install sklearn.linear_model\n",
    "!{sys.executable} -m pip install sklearn.impute\n",
    "!{sys.executable} -m pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ff6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for statistical tests\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import sklearn.preprocessing as preproc\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.impute as im\n",
    "\n",
    "## for date/time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab9725",
   "metadata": {},
   "source": [
    "## Dataset and its overall characteristics \n",
    "**Titanic dataset from Kaggle:**\n",
    "\n",
    "For the details see: https://www.kaggle.com/competitions/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d82c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/ZIFODS/Training/master/data/data_titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec257dc",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Adapted from https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e79b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show number of nulls (Not available data points or NAs) in the dataset\n",
    "missing = df.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700c7eb",
   "metadata": {},
   "source": [
    "There are missing values in columns Age (177), Cabin (687) and Embarked (2).\n",
    "\n",
    "Only some of the machine learning algorithms can work with missing data like KNN, which will ignore the values with Nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7537d",
   "metadata": {},
   "source": [
    "### Deleting the column with missing data\n",
    "This is an extreme case and should only be used when there are many null values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835e5670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Ticket       891 non-null    object \n",
      " 8   Fare         891 non-null    float64\n",
      "dtypes: float64(1), int64(5), object(3)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_df = df.dropna(axis=1).copy()\n",
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611c6a3",
   "metadata": {},
   "source": [
    "The problem with this method is that we may lose valuable information on the features, as we have deleted them completely due to some null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a021b",
   "metadata": {},
   "source": [
    "### Deleting the row with missing data\n",
    "If there is a certain row with missing data, then you can delete the entire row with all the features in that row.\n",
    "\n",
    "axis=1 is used to drop the column with `NaN` values.\n",
    "\n",
    "axis=0 is used to drop the row with `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb5ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 183 entries, 1 to 889\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  183 non-null    int64  \n",
      " 1   Survived     183 non-null    int64  \n",
      " 2   Pclass       183 non-null    int64  \n",
      " 3   Name         183 non-null    object \n",
      " 4   Sex          183 non-null    object \n",
      " 5   Age          183 non-null    float64\n",
      " 6   SibSp        183 non-null    int64  \n",
      " 7   Parch        183 non-null    int64  \n",
      " 8   Ticket       183 non-null    object \n",
      " 9   Fare         183 non-null    float64\n",
      " 10  Cabin        183 non-null    object \n",
      " 11  Embarked     183 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 18.6+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_df = df.dropna(axis=0).copy()\n",
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a8de2",
   "metadata": {},
   "source": [
    "We reduced number of records significantly. The better approach would be to get rid of the columns we are not that much interested in and that have null values (like Cabin and Embarked) and then apply the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1edd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 714 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  714 non-null    int64  \n",
      " 1   Survived     714 non-null    int64  \n",
      " 2   Pclass       714 non-null    int64  \n",
      " 3   Name         714 non-null    object \n",
      " 4   Sex          714 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        714 non-null    int64  \n",
      " 7   Parch        714 non-null    int64  \n",
      " 8   Ticket       714 non-null    object \n",
      " 9   Fare         714 non-null    float64\n",
      "dtypes: float64(2), int64(5), object(3)\n",
      "memory usage: 61.4+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_df = df.copy()\n",
    "updated_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Embarked\",axis=1,inplace=True)\n",
    "updated_df = updated_df.dropna(axis=0)\n",
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ea0ca",
   "metadata": {},
   "source": [
    "### Filling the Missing Values – Imputation\n",
    "\n",
    "In this case, we will be filling the missing values with a certain number.\n",
    "\n",
    "The possible ways to do this are:\n",
    "\n",
    "1. Filling the missing data with the mean or median value if it’s a numerical variable.\n",
    "2. Filling the missing data with mode if it’s a categorical value.\n",
    "3. Filling the numerical value with 0 or -999, or some other number that will not occur in the data. This can be done so that the machine can recognize that the data is not real or is different.\n",
    "\n",
    "You can use the fillna() function to fill the null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac646c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ad585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          891 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_df = df.copy()\n",
    "updated_df['Age']=updated_df['Age'].fillna(updated_df['Age'].mean())\n",
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9429d",
   "metadata": {},
   "source": [
    "### Filling with a Regression Model\n",
    "\n",
    "In this case, the null values in one column are filled by fitting a regression model using other columns in the dataset.\n",
    "\n",
    "I.E in this case the regression model will contain all the columns except Age in X and Age in Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e0caa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lm.LinearRegression()\n",
    "updated_df = df.copy()\n",
    "updated_df.drop(\"Name\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Ticket\",axis=1,inplace=True)\n",
    "updated_df.drop(\"PassengerId\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Embarked\",axis=1,inplace=True)\n",
    "le = preproc.LabelEncoder()\n",
    "updated_df['Sex'] = le.fit_transform(updated_df['Sex'])\n",
    "\n",
    "testdf = updated_df[updated_df['Age'].isnull()==True].copy()\n",
    "traindf = updated_df[updated_df['Age'].isnull()==False].copy()\n",
    "y = traindf['Age']\n",
    "traindf.drop(\"Age\",axis=1,inplace=True)\n",
    "testdf.drop(\"Age\",axis=1,inplace=True)\n",
    "lr.fit(traindf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d5d0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  SibSp  Parch     Fare\n",
       "5           0       3    1      0      0   8.4583\n",
       "17          1       2    1      0      0  13.0000\n",
       "19          1       3    0      0      0   7.2250\n",
       "26          0       3    1      0      0   7.2250\n",
       "28          1       3    0      0      0   7.8792\n",
       "..        ...     ...  ...    ...    ...      ...\n",
       "859         0       3    1      0      0   7.2292\n",
       "863         0       3    0      8      2  69.5500\n",
       "868         0       3    1      0      0   9.5000\n",
       "878         0       3    1      0      0   7.8958\n",
       "888         0       3    0      1      2  23.4500\n",
       "\n",
       "[177 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1ebed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aece9f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.07080066, 30.10833306, 22.44685065, 29.08927347, 22.43705181,\n",
       "       29.07922599, 32.43692984, 22.43898701, 22.15615704, 29.07922599,\n",
       "       29.07691632, 24.96460346, 22.43898701, 20.8713251 , 37.80993305,\n",
       "       44.85950626, 17.21443083, 29.07922599, 29.07691632, 22.43842532,\n",
       "       29.07691632, 29.07691632, 29.07922599, 22.14798185, 18.1926178 ,\n",
       "       29.07691632, 29.08140983, 17.39852793, 27.61791252, 29.08796287,\n",
       "       29.06774208, -5.49189866, 36.98755908, 44.88640441, 15.9929439 ,\n",
       "       -5.20126796, 37.01068094, 44.52580031, 18.32218064, 29.08140983,\n",
       "       22.43898701, -5.49189866, 25.08068578, 29.07922599, 16.2835746 ,\n",
       "       29.37503621, 25.27089854, 18.32218064, 29.08889901, 37.44600929,\n",
       "       29.08140983, 29.37204054, 44.81038921, 22.43898701, 37.23610531,\n",
       "       44.88528103, 44.85950626, 37.88482487, 22.43898701, 13.91474356,\n",
       "       30.40869971, 29.07691632, 36.97144531, -5.49189866, 14.20537427,\n",
       "       32.62971335, 29.07922599, 18.31319362, 44.75047576, 29.08927347,\n",
       "       22.43705181, 22.43705181, 24.96460346, 22.44678774, 22.43898701,\n",
       "       33.40078751, 29.07922599, 29.08172138, 16.2835746 , 29.07922599,\n",
       "       29.09476604, 37.23610531, 29.36754703, 29.07922599, 29.08889901,\n",
       "       29.08140983, 18.31319362, 22.14292665, 24.89769961, 29.07691632,\n",
       "       33.85475624, 29.08140983, 29.07691632, 37.23610531, 29.08178429,\n",
       "       29.08889901, 44.49584359, 37.23610531, 16.2835746 , 24.89769961,\n",
       "       28.98093048, 28.97131886, 29.37384993, 37.9439894 , 29.07691632,\n",
       "       28.83576467, 29.08927347, 29.08921056, 41.95282906, 29.08921056,\n",
       "       20.57832539, 28.98030588, 30.09541422, 29.08022355, 41.86676638,\n",
       "       29.08140983, 29.07691632, 29.36754703, 29.08921056, 22.43898701,\n",
       "       25.27083713, 29.06699316, 29.07691632, 27.73018734, 30.0993965 ,\n",
       "       29.08927347, 29.07922599, 44.63963587, 29.09189469, 18.32218064,\n",
       "       29.08140983, 29.08165997, 45.27471847, 24.95561644, 21.41822401,\n",
       "       29.0844055 , 29.07922599, 22.43780073, 29.07922599, 29.08103537,\n",
       "       33.85269671, 37.23610531, 29.36623642, 21.41822401, 22.43923715,\n",
       "       17.21443083, 44.87704293, 28.96532752, 22.43917424, 37.23610531,\n",
       "       29.07922599, 29.07922599, 37.89231405, 28.98030588, 44.68157529,\n",
       "       24.83504062, 29.08927347, 29.08140983, 29.08159706, 23.20367227,\n",
       "       29.08140983, -5.20126796, 44.81494563, 45.27471847, 29.09339252,\n",
       "       28.35127753, 22.1483563 , 29.08921056, 29.07691632, 37.89680756,\n",
       "       -5.49189866, 33.29693653, 29.08921056, -5.20126796, 29.0551977 ,\n",
       "       29.07922599, 23.49430298])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633e39b",
   "metadata": {},
   "source": [
    "### Filling with KNN\n",
    "\n",
    "KNN stands for the k-Nearest Neighbors method that is used to replace the missing values in the datasets with the mean value from the parameter ‘n_neighbors’ nearest neighbors found in the training set. By default, it uses a Euclidean distance metric to impute the missing values.\n",
    "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ddb3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = im.KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc11810",
   "metadata": {},
   "source": [
    "#### Textual Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b119786",
   "metadata": {},
   "source": [
    "One thing to note here is that the KNN Imputer does not recognize text data values. It will generate errors if we do not change these values to numerical values. For example, in our Titanic dataset, the categorical columns ‘Sex’ and ‘Embarked’ have text data.\n",
    "We are dropping most of the textual columns except for 'Sex' one. This column is transformed into numerical values using LabelEncoder from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9790eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc3ab11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19372/3385692145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cabin\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Embarked\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mupdated_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "updated_df = df.copy()\n",
    "updated_df.drop(\"Name\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Ticket\",axis=1,inplace=True)\n",
    "updated_df.drop(\"PassengerId\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Cabin\",axis=1,inplace=True)\n",
    "updated_df.drop(\"Embarked\",axis=1,inplace=True)\n",
    "le = LabelEncoder()\n",
    "updated_df['Sex'] = le.fit_transform(updated_df['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f88f74",
   "metadata": {},
   "source": [
    "#### Scaling (Normalisation)\n",
    "Another critical point here is that the KNN Imptuer is a distance-based imputation method and it requires us to **normalize** our data. Otherwise, the different scales of our data will lead the KNN Imputer to generate biased replacements for the missing values. For simplicity, we will use Scikit-Learn’s MinMaxScaler which will scale our variables to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.MinMaxScaler()\n",
    "updated_df = pd.DataFrame(scaler.fit_transform(updated_df), columns = updated_df.columns)\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64894783",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.DataFrame(imputer.fit_transform(updated_df),columns = updated_df.columns)\n",
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee650",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df['Age'].to_numpy(dtype=None, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea30562",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "Data transformation is the process of converting raw data into a a format or structure that would be more suitable for the model or algorithm and also data discovery in general. It is an essential step in the feature engineering that facilitates discovering insights. \n",
    "\n",
    "Numeric data transformation is needed when the data distribution is skewed since the ML algorithm is more likely to be biased in such a case. Besides, transforming data into the same scale allows the algorithm to compare the relative relationship between data points better.\n",
    "\n",
    "Adapted from https://www.visual-design.net/post/data-transformation-and-feature-engineering-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bf549",
   "metadata": {},
   "source": [
    "### Clipping\n",
    "\n",
    "This approach is more suitable when there are outliers in the dataset. Clipping method sets up the upper and lower bound and all data points will be contained within the range.\n",
    "\n",
    "We can use quantile() to find out what is the range of the majority amount of data (between 0.05 percentile and 0.95 percentile). Any numbers below the lower bound (defined by 0.05 percentile) will be rounded up to the lower bound. Similarly, the numbers above upper bound (defined by 0.95 percentile) will be rounded down to upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09546913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f84559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipping methods to handle outliers\n",
    "\n",
    "clip_var = ['Age', 'Fare']\n",
    "for i in clip_var:\n",
    "    transformed = 'clipped_' + i\n",
    "    # upper limit - 0.95 quantile\n",
    "    upper_limit = df[i].quantile(0.95)\n",
    "    # lower limit - 0.05 quantile\n",
    "    lower_limit = df[i].quantile(0.05)\n",
    "    df[transformed] = df[i].clip(lower_limit, upper_limit, axis = 0)\n",
    "    # print(df[i].describe())\n",
    "    # print(df[transformed].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab405a",
   "metadata": {},
   "source": [
    "### Basic transformation\n",
    "Basic data transformation is needed to make data tidier and more insightful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82edb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Income$': ['$150,000', '$10,800', '$120,0000', '$100,000'],\n",
    "    'Year_Birth': [2000, 2004, 1977, 1973],\n",
    "    'Joined': ['2018-12-19', '2022-1-23', '2010-10-3', '2001-5-30'],   \n",
    "    'Balance':[100.0, -263.0, 2000.0, -5.0],\n",
    "    'Department': ['HR','Legal','Marketing','Management']\n",
    "})\n",
    "\n",
    "df['Joined'] = pd.to_datetime(df['Joined'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of basic transformations:\n",
    "\n",
    "# 1. Transform Year of Birth into Age\n",
    "df['Age'] = dt.date.today().year - df['Year_Birth']\n",
    "\n",
    "# 2. Transform date into enrollment length:\n",
    "df['Enrollment_Length'] = ((pd.to_datetime(date.today()) - df['Joined'])/np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# 3. Transform Currency into numbers\n",
    "# This involves four steps: \n",
    "#   1) clean data to remove characters \", $ .\" \n",
    "#   2) substitute null value to 0; \n",
    "#   3) convert string into integer; \n",
    "#   4) scale down the numbers into thousand dollar which helps with visualizing the data distribution\n",
    "df['Income'] = df['Income$'].str.replace(',', '', regex=True).str.replace('$', '', regex=True).fillna(0).astype(int)\n",
    "#df['Income'] = df['Income'].apply(lambda x: round(x/1000))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f564a91",
   "metadata": {},
   "source": [
    "### Scaling Techniques\n",
    "\n",
    "Adapter from https://www.analyticsvidhya.com/blog/2020/07/types-of-feature-transformation-and-scaling/\n",
    "\n",
    "Often, we have datasets in which different columns have different units – like one column can be in kilograms, while another column can be in centimeters. Furthermore, we can have columns like income which can range from 20,000 to 100,000, and even more; while an age column which can range from 0 to 100. Thus, Income is about 1,000 times larger than age.\n",
    "\n",
    "When we feed these features to the model as is, there is every chance that the income will influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So, to give importance to both Age, and Income, we need feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b72b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before directly applying any feature transformation or scaling technique, \n",
    "# we need to remember the categorical columns and first deal with them. This is because we cannot scale non-numeric values.\n",
    "    \n",
    "df_scaled = df.copy()\n",
    "col_names = ['Income', 'Age', 'Enrollment_Length','Balance']\n",
    "features = df_scaled[col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2f0e3",
   "metadata": {},
   "source": [
    "#### MinMax Scaler\n",
    "The MinMax scaler is one of the simplest scalers to understand.  It just scales all the data between 0 and 1. \n",
    "\n",
    "The formula for calculating the scaled value is:\n",
    "\n",
    "**x_scaled = (x – x_min)/(x_max – x_min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6768b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.MinMaxScaler()\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we don’t want the income or age to have values like 0. Let us take the range to be (5, 10).\n",
    "scaler = MinMaxScaler(feature_range=(5, 10))\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e35eae",
   "metadata": {},
   "source": [
    "#### Standard Scaler\n",
    "For each feature, the Standard Scaler scales the values such that the mean is 0 and the standard deviation is 1.\n",
    "\n",
    "**x_scaled = x – mean/std_dev**\n",
    "\n",
    "However, Standard Scaler assumes that the distribution of the variable is normal. Thus, in case, the variables are not normally distributed, we\n",
    "\n",
    "1. either choose a different scaler\n",
    "2. or first, convert the variables to a normal distribution and then apply this scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.StandardScaler()\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()\n",
    "\n",
    "# The mean values of 'Income', 'Balance' and 'Age' are not exactly, but very close to 0 (1 in case of standard deviation). \n",
    "# This occurs due to the numerical precision of floating-point numbers in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bda167",
   "metadata": {},
   "source": [
    "#### MaxAbsScaler\n",
    "This scaler takes the absolute maximum value of each column and divides each value in the column by the maximum value.\n",
    "\n",
    "Thus, it first takes the absolute value of each value in the column and then takes the maximum value out of those. This operation scales the data between the range [-1, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.MaxAbsScaler()\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c80ade",
   "metadata": {},
   "source": [
    "#### Robust Scaler\n",
    "Each of the scalers we seen so far was using values like the mean, maximum and minimum values of the columns. All these values are sensitive to outliers. If there are too many outliers in the data, they will influence the mean and the max value or the min value. Thus, even if we scale this data using the above methods, we cannot guarantee a balanced data with a normal distribution.\n",
    "\n",
    "The Robust Scaler, as the name suggests is not sensitive to outliers. This scaler removes the median from the data and scales the data by the InterQuartile Range(IQR).\n",
    "\n",
    "IQR is the difference between the first and third quartile of the variable: **IQR = Q3 – Q1**\n",
    "\n",
    "Thus, the formula would be:\n",
    "\n",
    "**x_scaled = (x – Q1)/(Q3 – Q1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.RobustScaler()\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e330226",
   "metadata": {},
   "source": [
    "#### Quantile Transformer Scaler\n",
    "The Quantile Transformer Scaler converts the variable distribution to a normal distribution and scales it accordingly. \n",
    "\n",
    "Since it makes the variable normaly distributed, it also deals with the outliers. Here are a few important points regarding the Quantile Transformer Scaler:\n",
    "\n",
    "1. It computes the cumulative distribution function of the variable\n",
    "\n",
    "2. It uses this CDF to map the values to a normal distribution\n",
    "\n",
    "3. Maps the obtained values to the desired output distribution using the associated quantile function\n",
    "\n",
    "A caveat to keep in mind though: since this scaler changes the very distribution of the variables, linear relationships among variables may be destroyed by using this scaler. Thus, it is best to use this for non-linear data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.QuantileTransformer(n_quantiles=4)\n",
    "\n",
    "df_scaled[col_names] = scaler.fit_transform(features.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aab1b4",
   "metadata": {},
   "source": [
    "### Transformating non-normal data\n",
    "\n",
    "In some cases it is important that the data we have is of normal shape (also known as following a Bell curve). This includes regression analysis, the two-sample t-test, and Analysis of Variance (ANOVA), to name a few.\n",
    "\n",
    "\n",
    "Adapted from https://www.marsja.se/transform-skewed-data-using-square-root-log-box-cox-methods-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a222a",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis\n",
    "Briefly, skewness is a measure of lack of symmetry. This means that the larger the number is the more data lack symmetry (not normal, that is). Kurtosis, on the other hand, is a measure of whether data is heavy- or light-tailed relative to a normal distribution.\n",
    "\n",
    "**Fairly Symmetrical** Skewness:\t-0.5 to 0.5\n",
    "\n",
    "**Moderate Skewed**    Skewness:\t-0.5 to -1.0 and 0.5 to 1.0\n",
    "\n",
    "**Highly Swewed**      Skewness:\t< -1.0 and > 1.0\n",
    "\n",
    "\n",
    "There are also different statistical tests that can be used to test if data is normally distributed (Shapiro-Wilks test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_to_transform.csv')\n",
    "df.hist(grid=False,\n",
    "       figsize=(10, 6),\n",
    "       bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(['skew', 'kurtosis']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57298cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile Transformer is explained above\n",
    "scaler = preproc.QuantileTransformer()\n",
    "\n",
    "df.insert(len(df.columns), 'A_Quantile',\n",
    "         scaler.fit_transform(np.array(df.iloc[:,0]).reshape(-1,1)))\n",
    "df.insert(len(df.columns), 'B_Quantile',\n",
    "         scaler.fit_transform(np.array(df.iloc[:,1]).reshape(-1,1)))\n",
    "df.insert(len(df.columns), 'C_Quantile',\n",
    "         scaler.fit_transform(np.array(df.iloc[:,2]).reshape(-1,1)))\n",
    "df.insert(len(df.columns), 'D_Quantile',\n",
    "         scaler.fit_transform(np.array(df.iloc[:,3]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4757f",
   "metadata": {},
   "source": [
    "#### Square root transformation\n",
    "The square root method is typically used when your data is moderately skewed. \n",
    "Now using the square root (e.g., sqrt(x)) is  a transformation that has a moderate effect on distribution shape. It is generally used to reduce right skewed data. Finally, the square root can be applied on zero values and is most commonly used on counted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575738a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Square root transformation\n",
    "df.insert(len(df.columns), 'A_Sqrt',\n",
    "         np.sqrt(df.iloc[:,0]))\n",
    "\n",
    "# Square root transormation on left skewed data in Python:\n",
    "df.insert(len(df.columns), 'C_Sqrt',\n",
    "         np.sqrt(max(df.iloc[:, 2]+1) - df.iloc[:, 2])) # Here we have to reverse the distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bede4",
   "metadata": {},
   "source": [
    "#### Log transformation\n",
    "The logarithmic is a strong transformation that has a major effect on distribution shape. \n",
    "This technique is, as the square root method, oftenly used for reducing right skewness. \n",
    "It can not be applied to zero or negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d129ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python log transform\n",
    "df.insert(len(df.columns), 'B_log',\n",
    "         np.log(df['Highly Positive Skew']))\n",
    "\n",
    "\n",
    "df.insert(len(df.columns), 'C_log',\n",
    "         np.log(max(df.iloc[:, 2] + 1) - df.iloc[:, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b279ce",
   "metadata": {},
   "source": [
    "#### Box-Cox Transformation\n",
    "The Box Cox transformation is named after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed the technique.\n",
    "Formula:\n",
    "* y(λ) = (yλ – 1) / λ  if y ≠ 0\n",
    "* y(λ) = log(y)  if y = 0\n",
    "\n",
    "This is a procedure to identify a suitable exponent (Lambda) to use to transform skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox Transformation in Python\n",
    "df.insert(len(df.columns), 'A_Boxcox', \n",
    "              scipy.stats.boxcox(df.iloc[:, 0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(['skew']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539000c4",
   "metadata": {},
   "source": [
    "**NB!** If you get the “ValueError: Data must be positive” while using either np.sqrt(), np.log() or SciPy’s boxcox() it is because your dependent variable contains negative numbers. To solve this, you can reverse the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fee6e",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Load the \"Titanic\" dataset (url = 'https://raw.githubusercontent.com/ZIFODS/Training/master/data/data_titanic.csv').\n",
    "2. Delete the row with Age missing values. How many records remained?\n",
    "3. Apply clipping method to Age column using 0.1 and 0.8 quantiles for lower and upper limits. How many values are replaced with upper limit?\n",
    "4. What type of skewness column \"Fate\" has?\n",
    "5. Quantile Transform column \"Fare\". What value in the \"Fare\" column correspond to PassenderId equals 6 (rounded to two decimal places)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
